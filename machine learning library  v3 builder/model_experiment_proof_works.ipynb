{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import *\n",
    "from activation_functions import *\n",
    "from loss_functions import *\n",
    "from layers import *\n",
    "from model import *\n",
    "\n",
    "from itertools import product as iter_product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset_size_one_neuron = 10**5 \n",
    "learning_rate_one_neuron = float(10**-2)\n",
    "batch_size_one_neuron = 50\n",
    "x_range_one_neuron = [-1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def network_factory_one_neuron():\n",
    "#     return Neural_Network(\n",
    "#         num_input_neurons = 1,\n",
    "#         loss_function = MC_MSE(),\n",
    "#         batch_size = batch_size_one_neuron,\n",
    "#         learning_rate = learning_rate_one_neuron,\n",
    "#         layers = [\n",
    "#             Neural_Layer(1, Identity_Function()),\n",
    "#         ],\n",
    "#     )\\\n",
    "#         .set_full_validation(True)\\\n",
    "#         .compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_factory_one_neuron():\n",
    "    return Neural_Network_Momentum(\n",
    "        num_input_neurons = 1,\n",
    "        loss_function = MC_MSE(),\n",
    "        batch_size = batch_size_one_neuron,\n",
    "        learning_rate = learning_rate_one_neuron,\n",
    "        layers = [\n",
    "            Neural_Layer_Momentum(1, Identity_Function()),\n",
    "        ],\n",
    "        momentum_coefficient=0.9\n",
    "    )\\\n",
    "        .set_full_validation(True)\\\n",
    "        .compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network_one_neuron = network_factory_one_neuron()\n",
    "training_network_one_neuron = network_factory_one_neuron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heart surgery so provate attributes accessed\n",
    "target_network_one_neuron._layers[0]._weights_m = np.array([[3.0]])\n",
    "target_network_one_neuron._layers[0]._bias_v = np.array([5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_X_one_neuron = np.random.uniform(\n",
    "    low = x_range_one_neuron[0],\n",
    "    high = x_range_one_neuron[1],\n",
    "    size = (1, total_dataset_size_one_neuron)\n",
    ")\n",
    "data_set_Y_one_neuron = target_network_one_neuron.make_predicitons(data_set_X_one_neuron)\n",
    "\n",
    "data_set_one_neuron = [\n",
    "    data_set_X_one_neuron.reshape((total_dataset_size_one_neuron, 1)), \n",
    "    data_set_Y_one_neuron.reshape((total_dataset_size_one_neuron, 1)), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one_neuron = ML_Model(\n",
    "    neural_network=training_network_one_neuron,\n",
    "    whole_dataset_raw=data_set_one_neuron,\n",
    "    learning_rate_decay_rate=0.95,\n",
    "    learning_rage_decay_frequency=1000,\n",
    "    validation_batch_frequency=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.955711900033663"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_one_neuron.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, validation_losses, learning_rates = model_one_neuron.train(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(27.060554432629797, 0.0095),\n",
       " (23.26922106740657, 0.0095),\n",
       " (21.124311335313386, 0.0095),\n",
       " (19.543743274852957, 0.0095),\n",
       " (18.199613905413973, 0.0095),\n",
       " (16.974911053995495, 0.0095),\n",
       " (15.83143334891779, 0.0095),\n",
       " (14.766731011407817, 0.0095),\n",
       " (13.778344511599173, 0.0095),\n",
       " (12.85476521287692, 0.0095),\n",
       " (11.996979874671268, 0.009025),\n",
       " (11.229152601706256, 0.009025),\n",
       " (10.51242894669612, 0.009025),\n",
       " (9.84189707581336, 0.009025),\n",
       " (9.213980462955453, 0.00857375),\n",
       " (8.65506482985497, 0.00857375),\n",
       " (8.12997890257989, 0.00857375),\n",
       " (7.636636333198846, 0.00857375),\n",
       " (7.173281067989343, 0.00857375),\n",
       " (6.738373437959668, 0.00857375),\n",
       " (6.3297773297879445, 0.00857375),\n",
       " (5.945603879954991, 0.00857375),\n",
       " (5.58488169299284, 0.00857375),\n",
       " (5.245683660289143, 0.00857375),\n",
       " (4.9285162442766515, 0.0081450625),\n",
       " (4.643709803142617, 0.0081450625),\n",
       " (4.376250015631332, 0.0081450625),\n",
       " (4.123129817126552, 0.0081450625),\n",
       " (3.8846924490359975, 0.007737809374999999),\n",
       " (3.6714399599405683, 0.007737809374999999),\n",
       " (3.4697887484847687, 0.007737809374999999),\n",
       " (3.279373326712312, 0.007737809374999999),\n",
       " (3.099325413021227, 0.007737809374999999),\n",
       " (2.929111474712717, 0.007737809374999999),\n",
       " (2.7682116329756243, 0.007737809374999999),\n",
       " (2.6162705890362594, 0.007737809374999999),\n",
       " (2.4728346871875972, 0.007737809374999999),\n",
       " (2.336991860893875, 0.007737809374999999),\n",
       " (2.2086603670173486, 0.007350918906249998),\n",
       " (2.093264127501987, 0.007350918906249998),\n",
       " (1.984072457725454, 0.007350918906249998),\n",
       " (1.8802397695629212, 0.007350918906249998),\n",
       " (1.7820650467966095, 0.006983372960937498),\n",
       " (1.6935180764150735, 0.006983372960937498),\n",
       " (1.609419277729885, 0.006983372960937498),\n",
       " (1.5294273393318794, 0.006983372960937498),\n",
       " (1.4534883557645815, 0.006983372960937498),\n",
       " (1.3812496180042362, 0.006983372960937498),\n",
       " (1.3126263476666438, 0.006983372960937498),\n",
       " (1.2474167674908345, 0.006983372960937498),\n",
       " (1.1854772667780344, 0.006983372960937498),\n",
       " (1.1265770906707129, 0.006983372960937498),\n",
       " (1.0705971164814931, 0.006634204312890623),\n",
       " (1.0200032712153133, 0.006634204312890623),\n",
       " (0.9718390969411814, 0.006634204312890623),\n",
       " (0.9258952778582862, 0.006634204312890623),\n",
       " (0.8821484637490455, 0.006302494097246091),\n",
       " (0.8425182642441505, 0.006302494097246091),\n",
       " (0.8046599368240155, 0.006302494097246091),\n",
       " (0.7685053770357257, 0.006302494097246091),\n",
       " (0.7339574817432062, 0.006302494097246091),\n",
       " (0.70098103708696, 0.006302494097246091),\n",
       " (0.6694808878818053, 0.006302494097246091),\n",
       " (0.6393951387183457, 0.006302494097246091),\n",
       " (0.6107055739154972, 0.006302494097246091),\n",
       " (0.5832655272649813, 0.006302494097246091),\n",
       " (0.557018099635538, 0.005987369392383786),\n",
       " (0.5332120976457325, 0.005987369392383786),\n",
       " (0.5104568442061693, 0.005987369392383786),\n",
       " (0.48861529230972867, 0.005987369392383786),\n",
       " (0.46773623621078325, 0.005688000922764597),\n",
       " (0.44874032336763375, 0.005688000922764597),\n",
       " (0.43049252364362167, 0.005688000922764597),\n",
       " (0.41300305915977925, 0.005688000922764597),\n",
       " (0.39624595961299264, 0.005688000922764597),\n",
       " (0.38012291872615234, 0.005688000922764597),\n",
       " (0.3646745441025304, 0.005688000922764597),\n",
       " (0.34985226420679344, 0.005688000922764597),\n",
       " (0.3356437356206881, 0.005688000922764597),\n",
       " (0.3220177141343188, 0.005688000922764597),\n",
       " (0.3089148706326273, 0.005403600876626367),\n",
       " (0.29699329767473465, 0.005403600876626367),\n",
       " (0.28550041718972846, 0.005403600876626367),\n",
       " (0.2744749862301821, 0.005403600876626367),\n",
       " (0.2638617011505134, 0.005133420832795048),\n",
       " (0.2541735602189181, 0.005133420832795048),\n",
       " (0.2448387850314817, 0.005133420832795048),\n",
       " (0.23583328591919736, 0.005133420832795048),\n",
       " (0.22717436239191266, 0.005133420832795048),\n",
       " (0.21884210069893945, 0.005133420832795048),\n",
       " (0.2107873378562641, 0.005133420832795048),\n",
       " (0.20303937927738258, 0.005133420832795048),\n",
       " (0.19558223860975074, 0.005133420832795048),\n",
       " (0.18839274722334726, 0.005133420832795048),\n",
       " (0.18147308701056006, 0.0048767497911552955),\n",
       " (0.17513226017262692, 0.0048767497911552955),\n",
       " (0.16901340850027952, 0.0048767497911552955),\n",
       " (0.16310853211758236, 0.0048767497911552955),\n",
       " (0.15741048548366995, 0.00463291230159753),\n",
       " (0.15218901377175517, 0.00463291230159753),\n",
       " (0.14712962482824135, 0.00463291230159753),\n",
       " (0.14224090448429144, 0.00463291230159753),\n",
       " (0.13751686786584982, 0.00463291230159753),\n",
       " (0.1329512240253325, 0.00463291230159753),\n",
       " (0.1285331148981769, 0.00463291230159753),\n",
       " (0.12426399248931233, 0.00463291230159753),\n",
       " (0.12013873167733037, 0.00463291230159753),\n",
       " (0.11614699146950216, 0.00463291230159753),\n",
       " (0.11228891393925569, 0.0044012666865176535),\n",
       " (0.10874498046585791, 0.0044012666865176535),\n",
       " (0.10530946583116256, 0.0044012666865176535),\n",
       " (0.10198709408137199, 0.0044012666865176535),\n",
       " (0.09876519751630276, 0.004181203352191771),\n",
       " (0.09579875586392216, 0.004181203352191771),\n",
       " (0.09292255867299602, 0.004181203352191771),\n",
       " (0.09013265568145776, 0.004181203352191771),\n",
       " (0.08742825455117775, 0.004181203352191771),\n",
       " (0.08480409821856842, 0.004181203352191771),\n",
       " (0.08225823550252055, 0.004181203352191771),\n",
       " (0.0797870187606021, 0.004181203352191771),\n",
       " (0.07739250194938432, 0.004181203352191771),\n",
       " (0.07506910806676552, 0.004181203352191771),\n",
       " (0.07281492905203124, 0.003972143184582182),\n",
       " (0.07073674765350987, 0.003972143184582182),\n",
       " (0.06871822997303999, 0.003972143184582182),\n",
       " (0.06675649050481718, 0.003972143184582182),\n",
       " (0.0648513841476428, 0.0037735360253530726),\n",
       " (0.06309374627367334, 0.0037735360253530726),\n",
       " (0.061382891897463335, 0.0037735360253530726),\n",
       " (0.05971642443539063, 0.0037735360253530726),\n",
       " (0.05809833662092167, 0.0037735360253530726),\n",
       " (0.05652396221986229, 0.0037735360253530726),\n",
       " (0.05498805058359974, 0.0037735360253530726),\n",
       " (0.053495584887692064, 0.0037735360253530726),\n",
       " (0.05204368181114538, 0.0037735360253530726),\n",
       " (0.05063250843261215, 0.0037735360253530726),\n",
       " (0.04925743065791831, 0.0035848592240854188),\n",
       " (0.04798754409144793, 0.0035848592240854188),\n",
       " (0.046749528636413906, 0.0035848592240854188),\n",
       " (0.04554427644750229, 0.0035848592240854188)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(validation_losses, learning_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04438568672947555"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_one_neuron.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "Weights (1, 1):   [[3.0015919]]\n",
      "Bias (1,):   [4.78932903]\n"
     ]
    }
   ],
   "source": [
    "model_one_neuron\\\n",
    "    .get_neural_network()\\\n",
    "    .print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "Weights (1, 1):   [[0.39457444]]\n",
      "Bias (1,):   [0.1]\n"
     ]
    }
   ],
   "source": [
    "model_one_neuron\\\n",
    "    .get_neural_network()\\\n",
    "    .reset_parameters()\\\n",
    "    .print_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.16188704395285"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_one_neuron.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(momentum_coefficient, learning_rate, learning_rate_decay):\n",
    "    def network_factory_one_neuron():\n",
    "        return Neural_Network_Momentum(\n",
    "            num_input_neurons = 1,\n",
    "            loss_function = MC_MSE(),\n",
    "            batch_size = 50,\n",
    "            learning_rate = learning_rate,\n",
    "            layers = [\n",
    "                Neural_Layer_Momentum(1, Identity_Function()),\n",
    "            ],\n",
    "            momentum_coefficient=momentum_coefficient\n",
    "        )\\\n",
    "            .set_full_validation(True)\\\n",
    "            .compile()\n",
    "\n",
    "    target_network_one_neuron = network_factory_one_neuron()\n",
    "    training_network_one_neuron = network_factory_one_neuron()\n",
    "\n",
    "\n",
    "    total_dataset_size_one_neuron = 10**5\n",
    "    data_set_X_one_neuron = np.random.uniform(\n",
    "        low = -5.0,\n",
    "        high = 5.0,\n",
    "        size = (1, total_dataset_size_one_neuron)\n",
    "    )\n",
    "\n",
    "    pre_train_loss = -1\n",
    "    while pre_train_loss < 1:\n",
    "        target_network_one_neuron.reset_parameters()\n",
    "        data_set_Y_one_neuron = target_network_one_neuron.make_predicitons(data_set_X_one_neuron)\n",
    "\n",
    "        data_set_one_neuron = data_set_one_neuron = [\n",
    "            data_set_X_one_neuron.reshape((total_dataset_size_one_neuron, 1)), \n",
    "            data_set_Y_one_neuron.reshape((total_dataset_size_one_neuron, 1)), \n",
    "        ]\n",
    "\n",
    "        model_one_neuron = ML_Model(\n",
    "            neural_network=training_network_one_neuron,\n",
    "            whole_dataset_raw=data_set_one_neuron,\n",
    "            learning_rate_decay_rate=learning_rate_decay,\n",
    "            learning_rage_decay_frequency=100,\n",
    "            validation_batch_frequency=50,\n",
    "        )\n",
    "\n",
    "        pre_train_loss = model_one_neuron.test()\n",
    "\n",
    "\n",
    "\n",
    "    epochs, _, learning_rates = model_one_neuron.train(epochs=10)\n",
    "\n",
    "    post_train_loss = model_one_neuron.test()\n",
    "\n",
    "    return epochs, learning_rates[-1], pre_train_loss, post_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_momentum_coefficients = (\n",
    "    0.0, \n",
    "    0.5,\n",
    "    # 0.8,\n",
    "    0.9,\n",
    ")\n",
    "\n",
    "experimental_learning_rates = (float(10**-i) for i in range(1, 6))\n",
    "\n",
    "experimental_learning_rate_decay_rates = (\n",
    "    # (0.9)**0.1,\n",
    "    # (0.9)**0.01,\n",
    "    0.995,   \n",
    "    0.99,\n",
    "    0.98,\n",
    "    0.95,\n",
    "    0.9,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.1, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 6 epochs  with a final learning rate of 0.07292124703704618 had a loss reducuction of 6.637885879657491 to 3.2344276804999686e-24\n",
      "Experiment 1:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.1, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 7 epochs  with a final learning rate of 0.04753400420057071 had a loss reducuction of 2.973875450399666 to 9.4585096673535e-25\n",
      "Experiment 2:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.1, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 2 epochs  with a final learning rate of 0.092236816 had a loss reducuction of 5.35176476701673 to 1.731461271479776e-07\n",
      "Experiment 3:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.1, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.608599781118304e-05 had a loss reducuction of 1.054555557377572 to 3.7902023327741486e-14\n",
      "Experiment 4:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.1, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 3.9260092771818286e-08 had a loss reducuction of 24.53434459007546 to 1.0767935347569961e-09\n",
      "Experiment 5:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.01, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.0049571413690105036 had a loss reducuction of 1.4787328600079706 to 1.1844645609406383e-12\n",
      "Experiment 6:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.01, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.002448652990349295 had a loss reducuction of 8.672740786743313 to 2.3526640897774747e-11\n",
      "Experiment 7:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.01, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.0005910858963408706 had a loss reducuction of 16.624695745236288 to 3.4421708251580235e-09\n",
      "Experiment 8:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.01, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.6085997811183e-06 had a loss reducuction of 17.632387705930242 to 8.976299612685386e-12\n",
      "Experiment 9:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.01, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 3.926009277181832e-09 had a loss reducuction of 2.5259796970034687 to 7.309563759625766e-09\n",
      "Experiment 10:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.001, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.0004957141369010506 had a loss reducuction of 2.2237329720359846 to 6.515357779736235e-10\n",
      "Experiment 11:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.001, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.0002448652990349295 had a loss reducuction of 12.748638860562075 to 8.903239599037661e-10\n",
      "Experiment 12:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.001, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 5.910858963408708e-05 had a loss reducuction of 17.57450140086717 to 9.758814411080106e-09\n",
      "Experiment 13:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.001, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.608599781118303e-07 had a loss reducuction of 1.0449574738348186 to 1.7454886335742298e-11\n",
      "Experiment 14:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.001, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.387479093976241e-10 had a loss reducuction of 1.6689002362405492 to 2.896167345691795e-13\n",
      "Experiment 15:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.0001, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 4.9571413690105064e-05 had a loss reducuction of 7.562057736792705 to 5.52076502199285e-09\n",
      "Experiment 16:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.0001, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 2.4486529903492954e-05 had a loss reducuction of 14.56837905341991 to 7.232335122129614e-10\n",
      "Experiment 17:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.0001, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 5.910858963408706e-06 had a loss reducuction of 6.551734484433595 to 1.4229332372265012e-06\n",
      "Experiment 18:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.0001, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.608599781118306e-08 had a loss reducuction of 26.3200837667492 to 0.04791017688744669\n",
      "Experiment 19:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 0.0001, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 3.92600927718183e-11 had a loss reducuction of 2.7911937229307027 to 0.13740985926457333\n",
      "Experiment 20:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 1e-05, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 4.957141369010506e-06 had a loss reducuction of 18.691560397725322 to 0.6612529099253112\n",
      "Experiment 21:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 1e-05, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 2.4486529903492947e-06 had a loss reducuction of 4.222370645100783 to 0.3504023917055433\n",
      "Experiment 22:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 1e-05, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 5.910858963408708e-07 had a loss reducuction of 10.39258640765458 to 2.2489754974467986\n",
      "Experiment 23:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 1e-05, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.608599781118309e-09 had a loss reducuction of 2.7029815172836744 to 1.4356667096571623\n",
      "Experiment 24:\n",
      "experiment_parameters: {'momentum_coefficient': 0.0, 'learning_rate': 1e-05, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 3.926009277181825e-12 had a loss reducuction of 5.9299047297825425 to 4.393084720020133\n",
      "Experiment 25:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.1, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 6 epochs  with a final learning rate of 0.07402609576967048 had a loss reducuction of 5.24928919468023 to 3.656133989644059e-26\n",
      "Experiment 26:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.1, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 6 epochs  with a final learning rate of 0.05048858887870697 had a loss reducuction of 13.140456043054332 to 9.989256519825525e-24\n",
      "Experiment 27:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.1, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 9 epochs  with a final learning rate of 0.01083598327831057 had a loss reducuction of 2.386272370432804 to 4.936108224423565e-22\n",
      "Experiment 28:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.1, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.608599781118304e-05 had a loss reducuction of 6.046291365631543 to 1.2447867100256715e-13\n",
      "Experiment 29:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.1, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 3.9260092771818286e-08 had a loss reducuction of 11.616761875017229 to 7.808143781088779e-10\n",
      "Experiment 30:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.01, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.0049571413690105036 had a loss reducuction of 3.3329073582714934 to 2.323974251758708e-12\n",
      "Experiment 31:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.01, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.002448652990349295 had a loss reducuction of 1.0580716812369366 to 2.20869644196531e-12\n",
      "Experiment 32:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.01, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.0005910858963408706 had a loss reducuction of 8.14616212768167 to 1.6364752393982304e-10\n",
      "Experiment 33:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.01, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.6085997811183e-06 had a loss reducuction of 15.623512155887678 to 4.050488782890729e-09\n",
      "Experiment 34:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.01, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 3.926009277181832e-09 had a loss reducuction of 3.069471511246534 to 1.971180725517024e-08\n",
      "Experiment 35:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.001, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.0004957141369010506 had a loss reducuction of 3.303910206439565 to 1.6673091549504283e-10\n",
      "Experiment 36:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.001, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.0002448652990349295 had a loss reducuction of 29.759286351400394 to 1.1208213859435553e-08\n",
      "Experiment 37:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.001, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 5.910858963408708e-05 had a loss reducuction of 24.17237088094339 to 4.686560359454311e-10\n",
      "Experiment 38:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.001, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.608599781118303e-07 had a loss reducuction of 2.687965336071033 to 3.202627142517401e-10\n",
      "Experiment 39:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.001, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 9 epochs  with a final learning rate of 2.1514733098945908e-08 had a loss reducuction of 7.609824667201017 to 1.3489611263733625e-10\n",
      "Experiment 40:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.0001, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 4.9571413690105064e-05 had a loss reducuction of 3.4079320614985296 to 4.0037542011858594e-10\n",
      "Experiment 41:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.0001, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 2.4486529903492954e-05 had a loss reducuction of 1.8648105009479954 to 1.967420675790939e-10\n",
      "Experiment 42:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.0001, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 5.910858963408706e-06 had a loss reducuction of 14.280144984475495 to 4.005332898057692e-06\n",
      "Experiment 43:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.0001, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.608599781118306e-08 had a loss reducuction of 1.0648056238355166 to 0.0021013408839970407\n",
      "Experiment 44:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 0.0001, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 3.92600927718183e-11 had a loss reducuction of 9.357354470350035 to 0.49390749726062116\n",
      "Experiment 45:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 1e-05, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 4.957141369010506e-06 had a loss reducuction of 19.141083501836217 to 0.7254354630468325\n",
      "Experiment 46:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 1e-05, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 2.4486529903492947e-06 had a loss reducuction of 10.892554642274415 to 0.9371323556782837\n",
      "Experiment 47:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 1e-05, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 5.910858963408708e-07 had a loss reducuction of 16.32222282262575 to 3.652417487664723\n",
      "Experiment 48:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 1e-05, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.608599781118309e-09 had a loss reducuction of 15.484458382452091 to 8.333558018331539\n",
      "Experiment 49:\n",
      "experiment_parameters: {'momentum_coefficient': 0.5, 'learning_rate': 1e-05, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 3.926009277181825e-12 had a loss reducuction of 5.796537706147733 to 4.3248519766818125\n",
      "Experiment 50:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.1, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 7 epochs  with a final learning rate of 0.06970466008354953 had a loss reducuction of 1.8731140450217998 to 1.0444526797673055e-24\n",
      "Experiment 51:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.1, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 7 epochs  with a final learning rate of 0.04298890135238938 had a loss reducuction of 12.403497545607449 to 1.6882044578619746e-21\n",
      "Experiment 52:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.1, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.0059108589634087084 had a loss reducuction of 2.473733465429482 to 8.315532800900095e-20\n",
      "Experiment 53:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.1, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.608599781118304e-05 had a loss reducuction of 6.151716974163875 to 7.199439678221539e-11\n",
      "Experiment 54:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.1, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 3.9260092771818286e-08 had a loss reducuction of 12.383105538602688 to 2.6420544836141842e-09\n",
      "Experiment 55:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.01, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.0049571413690105036 had a loss reducuction of 1.3862569078805294 to 2.221043296452179e-11\n",
      "Experiment 56:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.01, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.002448652990349295 had a loss reducuction of 1.9209822173145823 to 2.531251368853873e-10\n",
      "Experiment 57:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.01, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.0005910858963408706 had a loss reducuction of 1.534141634339239 to 1.9628185487926617e-09\n",
      "Experiment 58:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.01, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.6085997811183e-06 had a loss reducuction of 2.32405842302848 to 4.0823716608534255e-08\n",
      "Experiment 59:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.01, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 3.926009277181832e-09 had a loss reducuction of 7.2232158164861495 to 1.0924194914846586e-07\n",
      "Experiment 60:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.001, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.0004957141369010506 had a loss reducuction of 1.053562032840466 to 2.896496349720205e-10\n",
      "Experiment 61:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.001, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 0.0002448652990349295 had a loss reducuction of 2.487409938814696 to 1.6682464217373832e-09\n",
      "Experiment 62:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.001, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 5.910858963408708e-05 had a loss reducuction of 8.525770567822699 to 1.5588024903006282e-08\n",
      "Experiment 63:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.001, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.608599781118303e-07 had a loss reducuction of 3.7787869739485958 to 2.280551844309094e-09\n",
      "Experiment 64:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.001, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 3.926009277181828e-10 had a loss reducuction of 18.86513293638294 to 6.833076071542259e-10\n",
      "Experiment 65:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.0001, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 4.9571413690105064e-05 had a loss reducuction of 22.38064332204779 to 5.189821667479038e-09\n",
      "Experiment 66:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.0001, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 2.4486529903492954e-05 had a loss reducuction of 9.568153333457166 to 1.1738909991265086e-08\n",
      "Experiment 67:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.0001, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 5.910858963408706e-06 had a loss reducuction of 7.556151724847799 to 2.264168513118933e-05\n",
      "Experiment 68:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.0001, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.608599781118306e-08 had a loss reducuction of 9.713866634374046 to 0.05295991376605181\n",
      "Experiment 69:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 0.0001, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 3.92600927718183e-11 had a loss reducuction of 2.561549653289416 to 0.21356420836069712\n",
      "Experiment 70:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 1e-05, 'learning_rate_decay': 0.995}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 4.957141369010506e-06 had a loss reducuction of 1.547218657042705 to 0.10045580049677293\n",
      "Experiment 71:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 1e-05, 'learning_rate_decay': 0.99}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 2.4486529903492947e-06 had a loss reducuction of 4.675667594971464 to 0.6072346843931976\n",
      "Experiment 72:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 1e-05, 'learning_rate_decay': 0.98}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 5.910858963408708e-07 had a loss reducuction of 3.086713920988584 to 0.8772675906214936\n",
      "Experiment 73:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 1e-05, 'learning_rate_decay': 0.95}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 7.608599781118309e-09 had a loss reducuction of 1.4062464015370846 to 0.8374230296099506\n",
      "Experiment 74:\n",
      "experiment_parameters: {'momentum_coefficient': 0.9, 'learning_rate': 1e-05, 'learning_rate_decay': 0.9}\n",
      "Experiment success: in 10 epochs  with a final learning rate of 3.926009277181825e-12 had a loss reducuction of 2.0281298302072823 to 1.5847718785078515\n"
     ]
    }
   ],
   "source": [
    "experiment_iterator = iter_product(\n",
    "    experimental_momentum_coefficients, experimental_learning_rates, experimental_learning_rate_decay_rates\n",
    ")\n",
    "\n",
    "for experiment_i, (momentum, learning_rate, learning_rate_decay) in enumerate(experiment_iterator):\n",
    "\n",
    "    print(f\"Experiment {experiment_i}:\")\n",
    "\n",
    "    experiment_parameters = dict(\n",
    "        momentum_coefficient=momentum, \n",
    "        learning_rate = learning_rate, \n",
    "        learning_rate_decay = learning_rate_decay\n",
    "    )\n",
    "\n",
    "    print(f\"experiment_parameters: {experiment_parameters}\")\n",
    "\n",
    "    try:\n",
    "        epochs, final_learning_rate, pre_train_loss, post_train_loss = experiment(\n",
    "            momentum_coefficient=momentum, \n",
    "            learning_rate = learning_rate, \n",
    "            learning_rate_decay = learning_rate_decay\n",
    "        )\n",
    "    except DiverganceError:\n",
    "        print(f\"Experiment failed divergance error\")\n",
    "    # except ParameterChangeZero:\n",
    "    #     print(f\"Experiment failed parameter change 0\")\n",
    "    else:\n",
    "        print(f\"Experiment success: in {epochs} epochs  with a final learning rate of {final_learning_rate} had a loss reducuction of {pre_train_loss} to {post_train_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
