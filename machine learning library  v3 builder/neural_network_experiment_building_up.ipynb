{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import *\n",
    "from activation_functions import *\n",
    "from loss_functions import *\n",
    "from layers import *\n",
    "import random\n",
    "\n",
    "# seed = 239435901\n",
    "# np.random.seed(seed)\n",
    "# random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_neuron_model = Neural_Network(\n",
    "    num_input_neurons = 1,\n",
    "    loss_function = MC_MSE(),\n",
    "    batch_size = 50,\n",
    "    learning_rate = 10**-1,\n",
    "    layers = [\n",
    "        Neural_Layer(1, Identity_Function()),\n",
    "    ]\n",
    ")\\\n",
    "    .set_full_validation(True)\\\n",
    "    .compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = (-4, 3)\n",
    "x_range = (-5, 5)\n",
    "\n",
    "def generate_data(size):\n",
    "    X = np.random.uniform(x_range[0], x_range[1], (1, size))\n",
    "    Y = coefficients[0] * X + coefficients[1]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124.25797924866286"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_neuron_model.evaluate_model_on_test_data(*generate_data(10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(10_000):\n",
    "#     single_neuron_model.train_model_on_minibatch(\n",
    "#         *generate_data(50)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126.53735296845707"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_neuron_model.evaluate_model_on_test_data(*generate_data(10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (1, 1):   [[-0.24260702]]\n",
      "Bias (1,):   [0.1]\n"
     ]
    }
   ],
   "source": [
    "single_neuron_model.get_layers()[0].print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_linear() -> Neural_Network:\n",
    "    model = Neural_Network(\n",
    "        num_input_neurons = 3,\n",
    "        loss_function = MC_MSE(),\n",
    "        batch_size = 5,\n",
    "        learning_rate = 10**-7,\n",
    "        layers = [\n",
    "            Neural_Layer(2, Identity_Function()),\n",
    "        ]\n",
    "    )\\\n",
    "        .set_full_validation(True)\\\n",
    "        .compile()\n",
    "    \n",
    "    assert model._num_input_neurons == 3\n",
    "    assert model._layers[0]._input_size == 3\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (2, 3):   [[ 1.6071369  -1.0380957  -1.15683498]\n",
      " [ 1.25064229 -2.09613302 -0.95908897]]\n",
      "Bias (2,):   [-0.32639901 -0.32639901]\n"
     ]
    }
   ],
   "source": [
    "target_linear_network = make_model_linear()\n",
    "\n",
    "# target_linear_network._layers[0]._bias_v = np.random.uniform(-3, 3, (3,))\n",
    "\n",
    "target_linear_network._layers[0]._bias_v = np.full((2,), random.uniform(-0.5, 0.5))\n",
    "target_linear_network._layers[0]._weights_m = np.random.uniform(-3, 3, (2,3))\n",
    "\n",
    "assert target_linear_network._layers[0]._bias_v.shape == target_linear_network._layers[0]._bias_v_dimensions\n",
    "assert target_linear_network._layers[0]._weights_m.shape == target_linear_network._layers[0]._wieghts_m_dimensions\n",
    "\n",
    "\n",
    "target_linear_network.get_layers()[0].print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_linear_network = make_model_linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = (-5, 5)\n",
    "\n",
    "def generate_data(size):\n",
    "    X = np.random.uniform(x_range[0], x_range[1], (3, size))\n",
    "    Y = target_linear_network.make_predicitons(X)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.600536151791644"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner_linear_network.evaluate_model_on_test_data(*generate_data(10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.42727441782147\n",
      "50.92150064903568\n",
      "55.68099327360042\n",
      "23.49771822959499\n",
      "41.87793057614715\n",
      "26.672075961546422\n",
      "40.702999026713066\n",
      "112.91011277983455\n",
      "87.44533678270092\n",
      "66.7594474983633\n",
      "68.86401135477256\n"
     ]
    }
   ],
   "source": [
    "for i in range(10**5):\n",
    "    loss = learner_linear_network.train_model_on_minibatch(\n",
    "        *generate_data(5)\n",
    "    )\n",
    "    if i % 10**4 == 0:\n",
    "        print(loss)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.174545300973115"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner_linear_network.evaluate_model_on_test_data(*generate_data(10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<loss_functions.MC_MSE at 0x1abcbe44510>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "input_size = 3\n",
    "output_size = 2\n",
    "\n",
    "target_layer = Neural_Layer(\n",
    "    number_neurons=output_size,\n",
    "    activation_function=Identity_Function()\n",
    ")\n",
    "\n",
    "\n",
    "training_layer = Neural_Layer(\n",
    "    number_neurons=output_size,\n",
    "    activation_function=Identity_Function()\n",
    ")\n",
    "\n",
    "learner_linear_network.reset_parameters()\n",
    "\n",
    "for layer in (target_layer, training_layer):\n",
    "    layer\\\n",
    "        .set_batch_size(batch_size)\\\n",
    "        .set_input_size(input_size)\\\n",
    "        .compile()\n",
    "\n",
    "\n",
    "\n",
    "def add_message(some_function, message):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(message)\n",
    "        return some_function(*args, **kwargs)\n",
    "    wrapper.__name__ = some_function.__name__\n",
    "    wrapper.__annotations__ = some_function.__annotations__\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_layer.initialise_random_parameters = add_message(training_layer.initialise_random_parameters, \"reseting training layer parameters\")\n",
    "target_layer.initialise_random_parameters = add_message(target_layer.initialise_random_parameters, \"reseting target layer parameters\")\n",
    "\n",
    "learner_linear_network._layers[0].initialise_random_parameters = add_message(\n",
    "    learner_linear_network._layers[0].initialise_random_parameters, \"reseting learner network first layer parameters\"\n",
    ")\n",
    "target_linear_network._layers[0].initialise_random_parameters = add_message(\n",
    "    target_linear_network._layers[0].initialise_random_parameters, \"reseting target network first layer parameters\"\n",
    ")\n",
    "\n",
    "\n",
    "loss_function = MC_MSE()\n",
    "\n",
    "loss_function\\\n",
    "    .set_batch_size(batch_size)\\\n",
    "    .set_vector_size(output_size)\\\n",
    "    .compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = (-5, 5)\n",
    "learning_rate = 2**-7\n",
    "# target_bias_range = (-0.5, 0.5)\n",
    "target_bias_range = (0.5, 0.5)\n",
    "\n",
    "\n",
    "target_layer._bias_v = target_linear_network._layers[0]._bias_v\n",
    "target_layer._weights_m = target_linear_network._layers[0]._weights_m\n",
    "training_layer._bias_v = learner_linear_network._layers[0]._bias_v\n",
    "training_layer._weights_m = learner_linear_network._layers[0]._weights_m\n",
    "\n",
    "\n",
    "assert (target_layer._bias_v == target_linear_network._layers[0]._bias_v).all()\n",
    "assert (target_layer._weights_m == target_linear_network._layers[0]._weights_m).all()\n",
    "assert (training_layer._bias_v == learner_linear_network._layers[0]._bias_v).all()\n",
    "assert (training_layer._weights_m == learner_linear_network._layers[0]._weights_m).all()\n",
    "\n",
    "target_layer.set_is_first_layer(True)\n",
    "training_layer.set_is_first_layer(True)\n",
    "\n",
    "assert target_layer._input_size == target_linear_network._layers[0]._input_size\n",
    "assert training_layer._input_size == learner_linear_network._layers[0]._input_size\n",
    "\n",
    "\n",
    "\n",
    "assert target_layer == target_linear_network._layers[0]\n",
    "assert training_layer == learner_linear_network._layers[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.4346986 , -2.07392311, -2.14960737, -0.84069387, -3.39811677],\n",
       "       [-1.82255328, -0.15211496, -3.3967375 ,  1.26602168, -1.06374915],\n",
       "       [-2.15907661,  3.61869114, -3.69000536, -1.08420678,  1.30589283]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.uniform(low=x_range[0], high=x_range[1], size=(input_size, batch_size))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 4.10769364,  4.67115005, -4.58840154,  4.02854707, -2.41245063],\n",
    "       [-3.97686015,  4.80541106,  4.37762451, -1.90813116,  1.07510244],\n",
    "       [-0.54072051,  4.69063895, -0.85679802,  2.87590576, -1.50484245]])\n",
    "\n",
    "Y_L = target_layer.foreward_propagate(X)\n",
    "\n",
    "\n",
    "# fails, not sure why\n",
    "# Y_N = learner_linear_network.make_predicitons(X)\n",
    "\n",
    "Y_N, _ = target_linear_network._foreward_propagate(X)\n",
    "\n",
    "assert (training_layer._bias_v == learner_linear_network._layers[0]._bias_v).all()\n",
    "assert (training_layer._weights_m == learner_linear_network._layers[0]._weights_m).all()\n",
    "\n",
    "\n",
    "assert Y_L.shape == Y_N.shape, f\"{Y_L.shape} != {Y_N.shape}\"\n",
    "assert np.equal(Y_L, Y_N).all()\n",
    "\n",
    "P_L = training_layer.foreward_propagate(X)\n",
    "P_N, _ = learner_linear_network._foreward_propagate(X)\n",
    "\n",
    "assert np.equal(P_L, P_N).all()\n",
    "\n",
    "\n",
    "loss_L = loss_function.compute_loss(P_L, Y_L)\n",
    "dldP_L = loss_function.compute_loss_gradient()\n",
    "\n",
    "loss_N = learner_linear_network._loss_function.compute_loss(P_L, Y_L)\n",
    "dldP_N = learner_linear_network._loss_function.compute_loss_gradient()\n",
    "\n",
    "assert np.equal(P_L, P_N).all()\n",
    "\n",
    "training_layer.back_propagate(dldP_L)\n",
    "\n",
    "learner_linear_network._foreward_propagate(X, Y_N)\n",
    "learner_linear_network._back_propogate()\n",
    "\n",
    "\n",
    "\n",
    "training_layer.update_parameters(learning_rate)\n",
    "learner_linear_network._update_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100: layer loss was 10.05995208 and network loss was 10.05995208\n",
      "iteration 200: layer loss was 0.55128104 and network loss was 0.55128104\n",
      "iteration 300: layer loss was 0.07743950 and network loss was 0.07743950\n",
      "iteration 400: layer loss was 0.05621966 and network loss was 0.05621966\n",
      "iteration 500: layer loss was 0.06168134 and network loss was 0.06168134\n",
      "iteration 600: layer loss was 0.02800586 and network loss was 0.02800586\n",
      "ending training as loss increasing\n",
      "iteration 700: layer loss was 0.03380906 and network loss was 0.03380906\n",
      "iteration 700: layer loss was 0.03380906 and network loss was 0.03380906\n"
     ]
    }
   ],
   "source": [
    "# rolling_window = 100\n",
    "# rolling_losses = [None for _ in range(rolling_window)]\n",
    "\n",
    "loss_check_rate = 50\n",
    "last_loss = None\n",
    "validation_data = generate_data(10000)\n",
    "max_iterations = 10**5\n",
    "loss_increased = False\n",
    "\n",
    "i = 0\n",
    "while i < max_iterations and not loss_increased:\n",
    "    i += 1\n",
    "\n",
    "    X = np.random.uniform(low=x_range[0], high=x_range[1], size=(input_size, batch_size))\n",
    "\n",
    "    Y_L = target_layer.foreward_propagate(X)\n",
    "\n",
    "\n",
    "    # fails, not sure why\n",
    "    # Y_N = learner_linear_network.make_predicitons(X)\n",
    "\n",
    "    Y_N, _ = target_linear_network._foreward_propagate(X)\n",
    "\n",
    "    assert (training_layer._bias_v == learner_linear_network._layers[0]._bias_v).all()\n",
    "    assert (training_layer._weights_m == learner_linear_network._layers[0]._weights_m).all()\n",
    "\n",
    "\n",
    "    assert Y_L.shape == Y_N.shape, f\"{Y_L.shape} != {Y_N.shape}\"\n",
    "    assert np.equal(Y_L, Y_N).all()\n",
    "\n",
    "    P_L = training_layer.foreward_propagate(X)\n",
    "    P_N, _ = learner_linear_network._foreward_propagate(X)\n",
    "\n",
    "    assert np.equal(P_L, P_N).all()\n",
    "\n",
    "\n",
    "    loss_L = loss_function.compute_loss(P_L, Y_L)\n",
    "    dldP_L = loss_function.compute_loss_gradient()\n",
    "\n",
    "    loss_N = learner_linear_network._loss_function.compute_loss(P_L, Y_L)\n",
    "    dldP_N = learner_linear_network._loss_function.compute_loss_gradient()\n",
    "\n",
    "    assert np.equal(P_L, P_N).all()\n",
    "\n",
    "    training_layer.back_propagate(dldP_L)\n",
    "\n",
    "    learner_linear_network._foreward_propagate(X, Y_N)\n",
    "    learner_linear_network._back_propogate()\n",
    "\n",
    "\n",
    "\n",
    "    training_layer.update_parameters(learning_rate)\n",
    "    learner_linear_network._update_parameters()\n",
    "\n",
    "    # if all(element is not None for element in rolling_losses):\n",
    "    #     previous_mean_loss = sum(rolling_losses) / rolling_window\n",
    "    #     rolling_losses[i%rolling_window] = learner_linear_network.evaluate_model_on_test_data(*generate_data(10_000))\n",
    "    #     new_mean_loss = sum(rolling_losses) / rolling_window\n",
    "    #     if new_mean_loss > previous_mean_loss:\n",
    "    #         print(\"ending training as loss increasing\")\n",
    "    #         break\n",
    "    # else:\n",
    "    #     rolling_losses[i%rolling_window] = learner_linear_network.evaluate_model_on_test_data(*generate_data(10_000))\n",
    "\n",
    "    if i % loss_check_rate == 0:\n",
    "        new_loss = learner_linear_network.evaluate_model_on_test_data(*validation_data)\n",
    "\n",
    "        if last_loss is not None:\n",
    "            if new_loss > last_loss:\n",
    "                print(\"ending training as loss increasing\")\n",
    "                loss_increased = True\n",
    "\n",
    "        last_loss = new_loss\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    if i % 10**2 == 0:\n",
    "        print(f\"iteration {i}: layer loss was {loss_L:.8f} and network loss was {loss_N:.8f}\")\n",
    "\n",
    "print(f\"iteration {i}: layer loss was {loss_L:.8f} and network loss was {loss_N:.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03352638316190613"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner_linear_network.evaluate_model_on_test_data(*generate_data(10_000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
