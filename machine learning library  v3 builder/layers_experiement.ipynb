{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from activation_functions import *\n",
    "from ml_exceptions import *\n",
    "from loss_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = Dropout_Layer(dropout_rate=0.4)\\\n",
    "    .set_batch_size(5)\\\n",
    "    .set_input_size(5)\\\n",
    "    .compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(-1, 1, (5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26323242,  0.83579902, -0.789571  , -0.17239793, -0.68753686],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.49257682,  0.9658885 , -0.57021181,  0.97732169, -0.38910908],\n",
       "       [ 0.06838749,  0.7968246 , -0.70529972,  0.94717784,  0.92977961],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout.foreward_propagate(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout.back_propagate(np.ones((5, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "linear_regression_model = Neural_Layer(\n",
    "    number_neurons=output_size,\n",
    "    activation_function=Identity_Function(),\n",
    ")\n",
    "linear_regression_model\\\n",
    "    .set_batch_size(batch_size)\\\n",
    "    .set_input_size(input_size)\\\n",
    "    .compile()\n",
    "\n",
    "\n",
    "loss_function = MC_MSE()\\\n",
    "    .set_vector_size(output_size)\\\n",
    "    .set_batch_size(batch_size)\\\n",
    "    .compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy supports broardcasting for simple functions\n",
    "def target_function(X):\n",
    "    return 3*X-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss was 211.40863644\n",
      "Weights (1, 1):   [[1.60271108]]\n",
      "Bias (1,):   [0.09760447]\n",
      "iteration 999: loss was 14.31758836\n",
      "Weights (1, 1):   [[3.05069413]]\n",
      "Bias (1,):   [-1.27011551]\n",
      "iteration 1999: loss was 7.46441743\n",
      "Weights (1, 1):   [[3.01012405]]\n",
      "Bias (1,):   [-2.2718899]\n",
      "iteration 2999: loss was 3.96328895\n",
      "Weights (1, 1):   [[3.02876215]]\n",
      "Bias (1,):   [-3.00442509]\n",
      "iteration 3999: loss was 2.11288890\n",
      "Weights (1, 1):   [[2.98297203]]\n",
      "Bias (1,):   [-3.54014353]\n",
      "iteration 4999: loss was 1.19515988\n",
      "Weights (1, 1):   [[3.0026349]]\n",
      "Bias (1,):   [-3.93198253]\n",
      "iteration 5999: loss was 0.61165700\n",
      "Weights (1, 1):   [[2.99435242]]\n",
      "Bias (1,):   [-4.21867254]\n",
      "iteration 6999: loss was 0.32930600\n",
      "Weights (1, 1):   [[2.99716888]]\n",
      "Bias (1,):   [-4.42855929]\n",
      "iteration 7999: loss was 0.17564925\n",
      "Weights (1, 1):   [[3.00197678]]\n",
      "Bias (1,):   [-4.58199846]\n",
      "iteration 8999: loss was 0.09438499\n",
      "Weights (1, 1):   [[2.99638574]]\n",
      "Bias (1,):   [-4.69419392]\n",
      "iteration 9999: loss was 0.04961663\n",
      "Weights (1, 1):   [[3.00266409]]\n",
      "Bias (1,):   [-4.77628698]\n"
     ]
    }
   ],
   "source": [
    "x_range = (-10, 10)\n",
    "learning_rate = 2**-8\n",
    "\n",
    "linear_regression_model.initialise_random_parameters()\n",
    "\n",
    "for i in range(10000):\n",
    "    X = np.random.uniform(low=x_range[0], high=x_range[1], size=(1, batch_size))\n",
    "    Y = target_function(X)\n",
    "    P = linear_regression_model.foreward_propagate(X)\n",
    "\n",
    "\n",
    "    loss = loss_function.compute_loss(P, Y)\n",
    "    dldP = loss_function.compute_loss_gradient()\n",
    "\n",
    "    linear_regression_model.back_propagate(dldP)\n",
    "    linear_regression_model.update_parameters(learning_rate)\n",
    "\n",
    "    if i==0 or (i+1) % 1000 == 0:\n",
    "        print(f\"iteration {i}: loss was {loss:.8f}\")\n",
    "        linear_regression_model.print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<loss_functions.MC_MSE at 0x22546928e10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "input_size = 3\n",
    "output_size = 2\n",
    "\n",
    "target_layer = Neural_Layer(\n",
    "    number_neurons=output_size,\n",
    "    activation_function=RELU()\n",
    ")\n",
    "training_layer = Neural_Layer(\n",
    "    number_neurons=output_size,\n",
    "    activation_function=RELU()\n",
    ")\n",
    "\n",
    "for layer in (target_layer, training_layer):\n",
    "    layer\\\n",
    "        .set_batch_size(batch_size)\\\n",
    "        .set_input_size(input_size)\\\n",
    "        .compile()\n",
    "\n",
    "loss_function = MC_MSE()\n",
    "\n",
    "\n",
    "loss_function\\\n",
    "    .set_batch_size(batch_size)\\\n",
    "    .set_vector_size(output_size)\\\n",
    "    .compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss was 42.95497457\n",
      "Weights error:\n",
      "[[ 1.10230619  0.66341568 -1.27300292]\n",
      " [ 1.73720566  2.37953754  2.73149331]]\n",
      "Bias error:\n",
      "[-0.39963806 -0.3994734 ]\n",
      "iteration 9999: loss was 0.00002576\n",
      "Weights error:\n",
      "[[-0.00066254  0.00018009  0.00581501]\n",
      " [-0.00025353 -0.000918   -0.00037847]]\n",
      "Bias error:\n",
      "[-0.01980682 -0.00221177]\n",
      "iteration 19999: loss was 0.00002989\n",
      "Weights error:\n",
      "[[-0.00044001  0.00063218  0.00507129]\n",
      " [ 0.00012181 -0.00055555 -0.00018791]]\n",
      "Bias error:\n",
      "[-0.01767181 -0.00026947]\n",
      "iteration 29999: loss was 0.00001767\n",
      "Weights error:\n",
      "[[-0.00139192  0.00075914  0.00441723]\n",
      " [ 0.00028055  0.00018318  0.00031821]]\n",
      "Bias error:\n",
      "[-0.0152989   0.00179921]\n",
      "iteration 39999: loss was 0.00002027\n",
      "Weights error:\n",
      "[[-0.00036307  0.00052979  0.00419521]\n",
      " [ 0.00048674  0.00069321  0.0011146 ]]\n",
      "Bias error:\n",
      "[-0.01392706  0.00295809]\n",
      "iteration 49999: loss was 0.00001202\n",
      "Weights error:\n",
      "[[-0.00061932  0.00038937  0.00335133]\n",
      " [ 0.00043585  0.00068568  0.00093053]]\n",
      "Bias error:\n",
      "[-0.01252942  0.00403829]\n",
      "iteration 59999: loss was 0.00001490\n",
      "Weights error:\n",
      "[[-0.00091705  0.00025792  0.00349005]\n",
      " [ 0.00049919  0.00085898  0.00152995]]\n",
      "Bias error:\n",
      "[-0.01136861  0.00487298]\n",
      "iteration 69999: loss was 0.00001230\n",
      "Weights error:\n",
      "[[-0.00027196  0.00032909  0.00332847]\n",
      " [ 0.00063524  0.00099034  0.00153583]]\n",
      "Bias error:\n",
      "[-0.01049102  0.00550952]\n",
      "iteration 79999: loss was 0.00000882\n",
      "Weights error:\n",
      "[[-0.0008861   0.00027762  0.00248395]\n",
      " [ 0.00027406  0.00049393  0.00154172]]\n",
      "Bias error:\n",
      "[-0.01005195  0.0057604 ]\n",
      "iteration 89999: loss was 0.00000791\n",
      "Weights error:\n",
      "[[-3.46452209e-04  2.98659381e-05  2.64175607e-03]\n",
      " [ 8.89337564e-04  1.83947766e-03  1.38094803e-03]]\n",
      "Bias error:\n",
      "[-0.00941628  0.00619205]\n",
      "iteration 99999: loss was 0.00000935\n",
      "Weights error:\n",
      "[[-5.03825526e-04 -8.19368113e-05  3.14134046e-03]\n",
      " [ 3.13703390e-05  8.10654191e-04  1.75720822e-03]]\n",
      "Bias error:\n",
      "[-0.00886487  0.00651285]\n"
     ]
    }
   ],
   "source": [
    "x_range = (-5, 5)\n",
    "learning_rate = 2**-6\n",
    "# target_bias_range = (-0.5, 0.5)\n",
    "target_bias_range = (0.5, 0.5)\n",
    "\n",
    "\n",
    "target_weight_range = (-3, 3)\n",
    "\n",
    "training_layer.initialise_random_parameters()\n",
    "target_layer.initialise_random_parameters(\n",
    "    bias_range=target_bias_range,\n",
    "    weight_range=target_weight_range\n",
    ")\n",
    "\n",
    "target_parameters = target_layer.get_parameters()\n",
    "\n",
    "\n",
    "for i in range(10**5):\n",
    "    X = np.random.uniform(low=x_range[0], high=x_range[1], size=(input_size, batch_size))\n",
    "    Y = target_layer.foreward_propagate(X)\n",
    "    P = training_layer.foreward_propagate(X)\n",
    "\n",
    "    loss = loss_function.compute_loss(P, Y)\n",
    "    dldP = loss_function.compute_loss_gradient()\n",
    "\n",
    "    training_layer.back_propagate(dldP)\n",
    "    training_layer.update_parameters(learning_rate)\n",
    "\n",
    "    if i==0 or (i+1) % 10**4 == 0:\n",
    "        print(f\"iteration {i}: loss was {loss:.8f}\")\n",
    "        \n",
    "        # if isinstance(loss_function, MVC_MSE):\n",
    "        #     mean_cost, variance_cost = loss_function.get_mean_cost_variance_cost()\n",
    "        #     print(f\"Mean was   {mean_cost:.8f}\\nvaraince cost was   {variance_cost:.8f}\\nprocessed varaince cost was   {(1+variance_cost)**variance_weighting:.8f}\")\n",
    "\n",
    "        # print(f\"dldP =   {dldP}\")\n",
    "\n",
    "        training_parameters = training_layer.get_parameters()\n",
    "        print(\"Weights error:\")\n",
    "        print(\n",
    "            training_parameters[\"W\"] - target_parameters[\"W\"]\n",
    "        )\n",
    "        print(\"Bias error:\")\n",
    "        print(\n",
    "            training_parameters[\"B\"] - target_parameters[\"B\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_training_statistics(mean_costs, variance_costs, num_minibatches=10_000):\n",
    "#     window_size=50\n",
    "#     # Calculate rolling averages\n",
    "#     rolling_mean_costs = np.convolve(mean_costs, np.ones(window_size)/window_size, mode='valid')\n",
    "#     rolling_variance_costs = np.convolve(variance_costs, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "#     # Plotting\n",
    "#     fig, ax1 = plt.subplots()\n",
    "\n",
    "#     # Set the limits of x and y axis\n",
    "#     ax1.set_xlim(0, num_minibatches)\n",
    "#     ax1.set_ylim(0, 1)\n",
    "    \n",
    "#     color = 'tab:red'\n",
    "#     ax1.set_xlabel('Mini-batch Number')\n",
    "#     ax1.set_ylabel('Mean Cost', color=color)\n",
    "#     ax1.plot(rolling_mean_costs, color=color)\n",
    "#     ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "#     ax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis\n",
    "\n",
    "#     # Set the limits for the second y axis\n",
    "#     ax2.set_ylim(0, 1)\n",
    "    \n",
    "#     color = 'tab:blue'\n",
    "#     ax2.set_ylabel('Variance of Cost', color=color)\n",
    "#     ax2.plot(rolling_variance_costs, color=color)\n",
    "#     ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "#     # Set spines to be at x=0, y=0\n",
    "#     ax1.spines['bottom'].set_position('zero')\n",
    "#     ax1.spines['left'].set_position('zero')\n",
    "\n",
    "#     # Remove top and right spines\n",
    "#     ax1.spines['top'].set_visible(False)\n",
    "#     ax1.spines['right'].set_visible(False)\n",
    "#     ax2.spines['top'].set_visible(False)\n",
    "#     ax2.spines['right'].set_visible(False)\n",
    "\n",
    "#     # Remove padding and margins\n",
    "#     plt.tight_layout(pad=0)\n",
    "\n",
    "#     # Set title\n",
    "#     plt.title(f\"Rolling Average of Mean Cost and Variance of Cost Over Training\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_experiement_data(variance_weighting, mean_weighting, learning_rate, num_minibatches=10_000):\n",
    "#     batch_size = 50\n",
    "#     input_size = 3\n",
    "#     output_size = 2\n",
    "\n",
    "#     target_layer = Neural_Layer(\n",
    "#         number_neurons=output_size,\n",
    "#         activation_function=RELU()\n",
    "#     )\n",
    "#     training_layer = Neural_Layer(\n",
    "#         number_neurons=output_size,\n",
    "#         activation_function=RELU()\n",
    "#     )\n",
    "\n",
    "#     for layer in (target_layer, training_layer):\n",
    "#         layer\\\n",
    "#             .set_batch_size(batch_size)\\\n",
    "#             .set_input_size(input_size)\\\n",
    "#             .compile()\n",
    "\n",
    "#     loss_function = MVC_MSE(\n",
    "#         variance_weighting = variance_weighting,\n",
    "#         mean_weighting=mean_weighting\n",
    "#     )\n",
    "\n",
    "\n",
    "#     loss_function\\\n",
    "#         .set_batch_size(batch_size)\\\n",
    "#         .set_vector_size(output_size)\\\n",
    "#         .compile()\n",
    "    \n",
    "#     x_range = (-5, 5)\n",
    "#     # target_bias_range = (-0.5, 0.5)\n",
    "#     target_bias_range = (0.5, 0.5)\n",
    "\n",
    "\n",
    "#     target_weight_range = (-3, 3)\n",
    "\n",
    "#     training_layer.initialise_random_parameters()\n",
    "#     target_layer.initialise_random_parameters(\n",
    "#         bias_range=target_bias_range,\n",
    "#         weight_range=target_weight_range\n",
    "#     )\n",
    "\n",
    "#     target_parameters = target_layer.get_parameters()\n",
    "\n",
    "#     varaince_cost_data = []\n",
    "#     mean_cost_data = []\n",
    "\n",
    "#     for i in range(num_minibatches):\n",
    "#         X = np.random.uniform(low=x_range[0], high=x_range[1], size=(input_size, batch_size))\n",
    "#         Y = target_layer.foreward_propagate(X)\n",
    "#         P = training_layer.foreward_propagate(X)\n",
    "\n",
    "#         loss = loss_function.compute_loss(P, Y)\n",
    "#         dldP = loss_function.compute_loss_gradient()\n",
    "\n",
    "#         training_layer.back_propagate(dldP)\n",
    "#         training_layer.update_parameters(learning_rate)\n",
    "\n",
    "#         mean_cost, variance_cost = loss_function.get_mean_cost_variance_cost()\n",
    "#         varaince_cost_data.append(variance_cost) \n",
    "#         mean_cost_data.append(mean_cost)\n",
    "\n",
    "#         # if i==0 or (i+1) % 5_000 == 0:\n",
    "#         #     print(f\"iteration {i}: loss was {loss:.8f}\")\n",
    "            \n",
    "#         #     print(f\"Mean was   {mean_cost:.8f}\\nvaraince cost was   {variance_cost:.8f}\\nprocessed varaince cost was   {(1+variance_cost)**variance_weighting:.8f}\")\n",
    "\n",
    "#         #     # print(f\"dldP =   {dldP}\")\n",
    "\n",
    "#         #     training_parameters = training_layer.get_parameters()\n",
    "#         #     print(\"Weights error:\")\n",
    "#         #     print(\n",
    "#         #         training_parameters[\"W\"] - target_parameters[\"W\"]\n",
    "#         #     )\n",
    "#         #     print(\"Bias error:\")\n",
    "#         #     print(\n",
    "#         #         training_parameters[\"B\"] - target_parameters[\"B\"]\n",
    "#         #     )\n",
    "\n",
    "#     return mean_cost_data, varaince_cost_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_training_statistics(\n",
    "#     *create_experiement_data(\n",
    "#         variance_weighting=-0.30,\n",
    "#         learning_rate = 2**-5\n",
    "#    )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_training_statistics(\n",
    "#     *create_experiement_data(\n",
    "#         variance_weighting=0,\n",
    "#         mean_weighting=1,\n",
    "#         learning_rate = 2**-5\n",
    "#    )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_training_statistics(\n",
    "#     *create_experiement_data(\n",
    "#         variance_weighting=1/4,\n",
    "#         mean_weighting=1,\n",
    "#         learning_rate = 2**-10\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_training_statistics(\n",
    "#     *create_experiement_data(\n",
    "#         variance_weighting=1/2,\n",
    "#         mean_weighting=1,\n",
    "#         learning_rate = 2**-8\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_training_statistics(\n",
    "#     *create_experiement_data(\n",
    "#         variance_weighting=3/4,\n",
    "#         mean_weighting=1,\n",
    "#         learning_rate = 2**-9\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_training_statistics(\n",
    "#     *create_experiement_data(\n",
    "#         variance_weighting=1,\n",
    "#         mean_weighting=1,\n",
    "#         learning_rate = 2**-10\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_training_statistics(\n",
    "#     *create_experiement_data(\n",
    "#         variance_weighting=1,\n",
    "#         mean_weighting=0,\n",
    "#         learning_rate=10**-4,\n",
    "#         num_minibatches=1_000_000\n",
    "#     ),\n",
    "#     num_minibatches=1_000_000\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
