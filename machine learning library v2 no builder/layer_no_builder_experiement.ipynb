{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer_no_builder import *\n",
    "from activation_functions_no_builder import *\n",
    "from loss_functions_no_builder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = Dropout_Layer(\n",
    "    dropout_rate=0.4,\n",
    "    input_size=5,\n",
    "    batch_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(-1, 1, (5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37001846,  0.85829505,  0.2978707 , -0.89282427,  0.3234147 ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.46658174, -0.17153959,  0.97117737,  0.14618221, -0.16652854],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.95436076, -0.47040459,  0.31758963, -0.44630288, -0.96960043]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout.foreward_propagate(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout.back_propagate(np.ones((5,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "linear_regression_model = Fully_Connected_Neuron_Layer(\n",
    "    input_size=1,\n",
    "    output_size=1,\n",
    "    activation_function=Identity_Function(batch_size=batch_size, vector_size=1),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "loss_function = MSE(batch_size=batch_size, vector_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy supports broardcasting for simple functions\n",
    "def target_function(X):\n",
    "    return 3*X-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_range = (-10, 10)\n",
    "# learning_rate = 2**-6\n",
    "\n",
    "# linear_regression_model.initialise_random_parameters()\n",
    "\n",
    "# for i in range(10000):\n",
    "#     X = np.random.uniform(low=x_range[0], high=x_range[1], size=(1, batch_size))\n",
    "#     Y = target_function(X)\n",
    "#     P = linear_regression_model.foreward_propagate(X)\n",
    "\n",
    "\n",
    "#     loss = loss_function.compute_loss(P, Y)\n",
    "#     dldP = loss_function.compute_loss_gradient()\n",
    "\n",
    "#     linear_regression_model.back_propagate(dldP)\n",
    "#     linear_regression_model.update_parameters(learning_rate)\n",
    "\n",
    "#     if i==0 or (i+1) % 1000 == 0:\n",
    "#         print(f\"iteration {i}: loss was {loss:.8f}\")\n",
    "#         linear_regression_model.print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "input_size = 3\n",
    "output_size = 2\n",
    "\n",
    "target_layer = Fully_Connected_Neuron_Layer(\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    activation_function=RELU(\n",
    "        batch_size=batch_size,\n",
    "        vector_size=output_size\n",
    "    ),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "training_layer = Fully_Connected_Neuron_Layer(\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    activation_function=RELU(\n",
    "        batch_size=batch_size,\n",
    "        vector_size=output_size\n",
    "    ),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "loss_function = MSE(\n",
    "    batch_size=batch_size,\n",
    "    vector_size=output_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss was 127.21357939\n",
      "Weights error:\n",
      "[[-2.66178441 -2.47883015  0.50535202]\n",
      " [ 2.07797698  1.97495475  2.92774632]]\n",
      "Bias error:\n",
      "[-0.03095982 -0.03191418]\n",
      "iteration 9999: loss was 0.04699409\n",
      "Weights error:\n",
      "[[-0.06981265 -0.02004332  0.02074537]\n",
      " [ 0.03649336  0.01593898  0.02733349]]\n",
      "Bias error:\n",
      "[0.31419604 0.29327876]\n",
      "iteration 19999: loss was 0.01187942\n",
      "Weights error:\n",
      "[[-0.04617233 -0.02898415  0.01486457]\n",
      " [ 0.02762894  0.02661421  0.03550363]]\n",
      "Bias error:\n",
      "[0.18167044 0.16296107]\n",
      "iteration 29999: loss was 0.00428803\n",
      "Weights error:\n",
      "[[-0.02818211 -0.01354633  0.0027573 ]\n",
      " [ 0.01674435  0.02094584  0.01479872]]\n",
      "Bias error:\n",
      "[0.11110508 0.09254198]\n",
      "iteration 39999: loss was 0.00159046\n",
      "Weights error:\n",
      "[[-0.01857873 -0.0066604  -0.00120781]\n",
      " [ 0.0006772   0.00836235  0.01515734]]\n",
      "Bias error:\n",
      "[0.06988443 0.05153776]\n",
      "iteration 49999: loss was 0.00053388\n",
      "Weights error:\n",
      "[[-0.01205841 -0.00637869 -0.00029578]\n",
      " [ 0.00672747  0.00733245  0.00455032]]\n",
      "Bias error:\n",
      "[0.04694786 0.02906805]\n",
      "iteration 59999: loss was 0.00020666\n",
      "Weights error:\n",
      "[[-0.00630267 -0.00404119 -0.00090134]\n",
      " [ 0.00364418  0.00142348  0.00116075]]\n",
      "Bias error:\n",
      "[0.02996844 0.01301884]\n",
      "iteration 69999: loss was 0.00010892\n",
      "Weights error:\n",
      "[[-6.73429735e-03 -1.94364323e-03  5.11418353e-04]\n",
      " [ 7.33885080e-04 -1.01648664e-04  8.66526746e-05]]\n",
      "Bias error:\n",
      "[0.02056651 0.0042381 ]\n",
      "iteration 79999: loss was 0.00003979\n",
      "Weights error:\n",
      "[[-0.00461137 -0.00387446  0.00104823]\n",
      " [-0.00054961 -0.00040486 -0.00062053]]\n",
      "Bias error:\n",
      "[0.01620559 0.00037066]\n",
      "iteration 89999: loss was 0.00002176\n",
      "Weights error:\n",
      "[[-3.80974973e-03 -1.30152690e-03  2.75929672e-04]\n",
      " [-1.00841002e-05 -2.04364801e-04 -6.13784679e-04]]\n",
      "Bias error:\n",
      "[ 0.01245538 -0.00301497]\n",
      "iteration 99999: loss was 0.00002864\n",
      "Weights error:\n",
      "[[-2.66194753e-03 -1.27362667e-03  4.65764611e-05]\n",
      " [-4.91055816e-04 -2.92626614e-04 -8.23856566e-04]]\n",
      "Bias error:\n",
      "[ 0.01076649 -0.00421839]\n"
     ]
    }
   ],
   "source": [
    "x_range = (-5, 5)\n",
    "learning_rate = 2**-6\n",
    "target_bias_range = (-0.5, 0.5)\n",
    "target_weight_range = (-3, 3)\n",
    "\n",
    "training_layer.initialise_random_parameters()\n",
    "target_layer.initialise_random_parameters(\n",
    "    bias_range=target_bias_range,\n",
    "    weight_range=target_weight_range\n",
    ")\n",
    "\n",
    "target_parameters = target_layer.get_parameters()\n",
    "\n",
    "for i in range(10**5):\n",
    "    X = np.random.uniform(low=x_range[0], high=x_range[1], size=(input_size, batch_size))\n",
    "    Y = target_layer.foreward_propagate(X)\n",
    "    P = training_layer.foreward_propagate(X)\n",
    "\n",
    "    loss = loss_function.compute_loss(P, Y)\n",
    "    dldP = loss_function.compute_loss_gradient()\n",
    "\n",
    "    training_layer.back_propagate(dldP)\n",
    "    training_layer.update_parameters(learning_rate)\n",
    "\n",
    "    if i==0 or (i+1) % 10**4 == 0:\n",
    "        print(f\"iteration {i}: loss was {loss:.8f}\")\n",
    "        training_parameters = training_layer.get_parameters()\n",
    "        print(\"Weights error:\")\n",
    "        print(\n",
    "            training_parameters[\"W\"] - target_parameters[\"W\"]\n",
    "        )\n",
    "        print(\"Bias error:\")\n",
    "        print(\n",
    "            training_parameters[\"B\"] - target_parameters[\"B\"]\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
